<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title></title>
    <link>http://collinjennings.com</link>
    <description>
      
    </description>
    
        
            <item>
                <title>Creating Computer-Generated Poetry</title>
                <link>http://collinjennings.com/2016/10/31/2016-text-generation/</link>
                <content:encoded>
                    <![CDATA[
                    <p>This is the notebook that Sven Anderson and I used for our “Family Weekend” class over the weekend. We began the class with two poems from an NPR article called, “<a href="http://www.npr.org/sections/alltechconsidered/2016/06/27/480639265/human-or-machine-can-you-tell-who-wrote-these-poems">Human or Machine: Can you tell who wrote these poems?</a>”</p>

<p>The question of what constitutes poetic style has and will continue to produce varied and complicated answers, but, at bottom, it comes down to choice. This experiment in generating “poetry” represents one way of simulating this process of choice by randomly picking words according to their frequency in a particular poet’s corpus. (Just to note, some of the functions in this script are in a separate file (<code class="highlighter-rouge">functions.py</code>) to make it easier to read. You can access those functions and the accompanying text files <a href="https://www.dropbox.com/sh/zme9hxf35ewdtvm/AADiV4PHy7i0xtF8uOBl__Xfa?dl=0">here</a>.)</p>

<h3 id="objective-we-will-be-able-to-articulate-specific-features-of-poetic-writing-that-can-distinguish-computer-generated-poems-from-ones-written-by-people">Objective: We will be able to articulate specific features of poetic writing that can distinguish computer-generated poems from ones written by people.</h3>

<p>In this class, we’ll be working with python code in this <a href="http://jupyter.org">Jupyter notebook</a> interface. What’s great about this platform is that we can easily move between formatted text (as illustrated here), code, images, and pretty much any other type of digital object.</p>

<p>We will use this notebook to produce a randomly-generated poem based on a particular poet’s sytle.</p>

<p><img src="/assets/wordCloud.png" alt="lincolnCloud" />
A word cloud generated from Abraham Lincoln’s <em>Gettysburg Address</em>. The size of the words correspond to their frequency in the speech.</p>

<h4 id="how-might-we-mathematically-represent-authorial-choice">How might we mathematically represent authorial choice?</h4>
<p>We can start by attempting to simulate the process by which particular authors make stylistic choices – for instance, how they choose which words to use.</p>

<p>In programming, we produce simulations using <strong>algorithms</strong>, which are sets of rules for the computer to follow. They are basically computational recipes for turning a given input into an output. Here we create an algorithm in a <strong>function</strong> for calculating the tip for a meal.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">tipCalculator</span> <span class="p">(</span><span class="n">mealTotal</span><span class="p">):</span> <span class="c">### Our "mealTotal" is our input. </span>
    <span class="k">return</span> <span class="n">mealTotal</span> <span class="o">*</span> <span class="mf">0.20</span> <span class="o">+</span> <span class="n">mealTotal</span> <span class="c">### We add 20% of the total to the input to get our output</span>

<span class="c">## We've created the function, now we can run it: </span>
<span class="k">print</span><span class="p">(</span><span class="n">tipCalculator</span><span class="p">(</span><span class="mf">50.00</span><span class="p">))</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>60.0
</code></pre>
</div>

<h4 id="now-lets-think-about-how-we-might-simulate-poetic-word-choice-to-produce-randomly-generated-poetry">Now let’s think about how we might simulate poetic word choice to produce randomly generated poetry.</h4>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Load some functionality and some pre-computed probabilities.</span>
<span class="kn">import</span> <span class="nn">functions</span> 
<span class="kn">import</span> <span class="nn">pickle</span> 
<span class="n">dickinsonProbs</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s">'dickinsonProbs.p'</span><span class="p">,</span> <span class="s">'rb'</span><span class="p">))</span>
</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">dickinsonProbs</span><span class="p">[</span><span class="s">'the'</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">dickinsonProbs</span><span class="p">[</span><span class="s">'heart'</span><span class="p">])</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>0.06506088945704097
0.0009419363520150709
</code></pre>
</div>

<p>These numbers tell us the likelihood of picking each word if we were to draw them at random from the text: the more frequent the word in the text, the greater chance of choosing it.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">random</span> <span class="kn">import</span> <span class="n">random</span> 
<span class="k">def</span> <span class="nf">randomWord</span><span class="p">(</span><span class="n">probs</span><span class="p">):</span>
    <span class="s">'''Given probs, a dictionary of word probabilities, this
    returns a word according to how frequently that word is found
    in the dictionary.'''</span>
    <span class="n">rnum</span> <span class="o">=</span> <span class="n">random</span><span class="p">()</span>
    <span class="n">sumprob</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">probs</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="n">sumprob</span> <span class="o">+=</span> <span class="n">probs</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">sumprob</span> <span class="o">&gt;</span> <span class="n">rnum</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">k</span>
    <span class="k">return</span> <span class="n">k</span>
</code></pre>
</div>

<p>We can simulate this process by using a function that randomly chooses words from Dickinson’s poetry, based on her word usage.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">randomWord</span><span class="p">(</span><span class="n">dickinsonProbs</span><span class="p">))</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>drum
</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># what happens if 10 is replaced by 20?</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span> 
    <span class="k">print</span><span class="p">(</span><span class="n">randomWord</span><span class="p">(</span><span class="n">dickinsonProbs</span><span class="p">))</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>since
but
them
most
few
sinks
one
repeal
the
delight
</code></pre>
</div>

<p>This frequency-based approach gives us a way to simulate the poet’s process of choosing words. But after we choose a random word what comes next?</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">dickinsonBigrams</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s">'dickinsonBigrams.p'</span><span class="p">,</span> <span class="s">'rb'</span><span class="p">))</span>
<span class="n">shakespeareProbs</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s">'shakeProbs.p'</span><span class="p">,</span> <span class="s">'rb'</span><span class="p">))</span>
<span class="n">shakespeareBigrams</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s">'shakeBigrams.p'</span><span class="p">,</span> <span class="s">'rb'</span><span class="p">))</span>
</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">shakespeareBigrams</span><span class="p">[</span><span class="s">'compare'</span><span class="p">])</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="p">{</span><span class="err">'thou':</span><span class="w"> </span><span class="err">0.3333333333333333,</span><span class="w"> </span><span class="err">'with':</span><span class="w"> </span><span class="err">0.16666666666666666,</span><span class="w"> </span><span class="err">'myself':</span><span class="w"> </span><span class="err">0.16666666666666666,</span><span class="w"> </span><span class="err">'thee':</span><span class="w"> </span><span class="err">0.16666666666666666,</span><span class="w"> </span><span class="err">'them':</span><span class="w"> </span><span class="err">0.16666666666666666</span><span class="p">}</span><span class="w">
</span></code></pre>
</div>

<p>We can choose the next word based on which words are likely to follow the first word, according to the poet’s usage. In Shakespeares’s Sonnets, <em>thou</em> is the most likely word to follow <em>compare</em>.</p>

<p>With this process of randomly choosing words likely to follow one another we can build entire poems. In the following algorithm, we combine this process with a method of making the poems rhyme by matching the last word of every odd line with a random rhyming word in every even line.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">generatePoem</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">bigrams</span><span class="p">,</span> <span class="n">lineLength</span><span class="p">,</span> <span class="n">poemLength</span><span class="p">):</span>
    <span class="n">poemLines</span> <span class="o">=</span> <span class="p">[]</span> 
    <span class="c"># create poemLines from probabilities</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">poemLength</span><span class="p">):</span>
        <span class="n">line</span> <span class="o">=</span> <span class="n">functions</span><span class="o">.</span><span class="n">generateFromBigrams</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">bigrams</span><span class="p">,</span> <span class="n">lineLength</span><span class="p">)</span>
        <span class="n">poemLines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
    <span class="n">lineCount</span> <span class="o">=</span> <span class="mi">1</span> 
    <span class="n">newLines</span> <span class="o">=</span> <span class="p">[]</span> 
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">poemLines</span><span class="p">:</span> 
        <span class="k">if</span> <span class="n">lineCount</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span> <span class="c"># an odd line sets the rhyme</span>
            <span class="n">rhymedLine</span> <span class="o">=</span> <span class="n">line</span> 
            <span class="n">newLines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rhymedLine</span><span class="p">)</span>
            <span class="n">lineCount</span> <span class="o">+=</span> <span class="mi">1</span> 
        <span class="k">elif</span> <span class="n">lineCount</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> <span class="c"># an even line must match rhyme of </span>
            <span class="n">rhymingLine</span> <span class="o">=</span> <span class="n">line</span>  <span class="c"># preceding line</span>
            <span class="k">try</span><span class="p">:</span> 
                <span class="k">if</span> <span class="n">rhymedLine</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">in</span> <span class="n">functions</span><span class="o">.</span><span class="n">pronounce</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span> 
                    <span class="n">newWord</span> <span class="o">=</span> <span class="n">functions</span><span class="o">.</span><span class="n">rhyme</span><span class="p">(</span><span class="n">rhymedLine</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">3</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
                    <span class="n">rhymingLine</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">newWord</span> <span class="c"># fix the rhyme to match</span>
                    <span class="n">newLines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rhymingLine</span><span class="p">)</span>
                    <span class="n">lineCount</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">except</span><span class="p">:</span> 
                <span class="n">newLines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rhymingLine</span><span class="p">)</span>
                <span class="n">lineCount</span> <span class="o">+=</span> <span class="mi">1</span> 
    <span class="c"># Now we concatenate to a single string.</span>
    <span class="n">fullPoem</span> <span class="o">=</span> <span class="p">[]</span> 
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">newLines</span><span class="p">:</span>
        <span class="n">line</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
        <span class="n">fullLine</span> <span class="o">=</span> <span class="s">' '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">line</span><span class="p">)</span>
        <span class="n">fullPoem</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fullLine</span><span class="p">)</span>
    <span class="k">return</span> <span class="s">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">line</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">fullPoem</span><span class="p">)</span>
</code></pre>
</div>

<p>Now we can generate our own “poems” from the word choice of either Shakespeare or Emily Dickinson.</p>

<h3 id="generate-shakespearean-poetry">Generate Shakespearean poetry</h3>

<p>For this code to work, change the hashtags to numbers – the first for the number of words you want in each line and the second for the number of lines you want in the poem.</p>

<p>(I really like the concluding couplet of the Shakespearean poem.)</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">generatePoem</span><span class="p">(</span><span class="n">shakespeareProbs</span><span class="p">,</span> <span class="n">shakespeareBigrams</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">14</span> <span class="p">))</span>
</code></pre>
</div>
<div class="highlighter-rouge"><pre class="highlight"><code>praises worse than at that i not so affined 
that our time and eyes have might to give 
in my bosoms ward but let me do forgive 
yore those children nursed deliverd from serving with thee 
which in the strength and they look what thee 
from the first the morning have been mine eye 
thy fair assistance in loves fire shall in ai 
yet the perfumed tincture of my desire keep open 
they see barren tender feeling but yet eyes alagappan 
doubting the face should transport me that is kind 
enforced to my deepest sense to his brief affined 
and gives thee back the past i have often 
not my love thy store when days when acetaminophen 
</code></pre>
</div>

<h3 id="generate-dickinsonian-poery">Generate Dickinsonian Poery</h3>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">generatePoem</span><span class="p">(</span><span class="n">dickinsonProbs</span><span class="p">,</span> <span class="n">dickinsonBigrams</span><span class="p">,</span> <span class="c">##, ##))</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>for heaven she had come nor simplified her 
consummate plush how cold i had the her 
off for pearl then of love is finished 
you lost was there came out time abolished 
held but internal evidence gives us is overcome 
he stayed away upon the merchant smiled branscome 
</code></pre>
</div>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/2016/10/31/2016-text-generation/</guid>
                <description>
                    
                </description>
                <pubDate>Mon, 31 Oct 2016 10:27:24 -0400</pubDate>
                <author>Collin Jennings</author>
            </item>
        
    
        
            <item>
                <title>Privacy Performed at Scale</title>
                <link>http://collinjennings.com/2016/10/04/privacy-performed/</link>
                <content:encoded>
                    <![CDATA[
                    <p>You wear a Fitbit during your jog in the morning, swipe your grocery loyalty card when you checkout in the evening, and share your contacts when you play Candy Crush at night. This “Internet of Things” offers automation, customization, and convenience and, in turn, demands access. The daily choices to allow access or not repeatedly require you to produce the contours of your private life, to open certain spaces and keep others closed. In this collaborative potluck, we asked: how do those contours take shape? (If your first question, is what the heck is a critical potluck? In short, it’s a workshop-like collaborative event; this one was held on September 15 at Bard. Check our handy <a href="/criticalPotluck.html">guide</a> for more details.)</p>

<p>We began with an object of inquiry: Part 4 of the <a href="http://www.pewinternet.org/datasets/jan-27-feb-16-2015-privacy-panel-4/">Pew Research Center’s Internet Survey</a>. It was conducted on 461 participants between Jan. 27 and Feb. 16, 2015. The survey offers a snapshot of how people face the ever-growing number of technologies and products that make claims upon various types of private information (from your Google search history to your average driving speed). Pew released the survey data with <a href="http://www.pewinternet.org/2016/01/14/privacy-and-information-sharing/">summary analysis</a> that provides a glimpse of the general sentiments of respondents. (Hover over the pie charts to see the corresponding questions.)</p>

<p><img src="/assets/retLoyPie.png" alt="retail" title="A grocery store has offered you a free loyalty card that will save you money on your purchases. In exchange the store will keep track of your shopping habits and sell this data to third parties. Would this scenario be acceptable to you, or not? " /><img src="/assets/smartThermoPie2.png" alt="thermo" title="A new technology company has created an inexpensive thermostat sensor for your house that would learn about your temperature zone and movements around the house and potentially save you on your energy bill. It is programmable remotely in return for sharing data about some of the basic activities that take place in your house like when people are there and when they move from room to room. Would this be acceptable or not?" /><img src="/assets/surCamPie.png" alt="surveil" title="Several co-workers of yours have recently had personal belongings stolen from your workplace, and the company is planning to instal high-resolution security cameras that use facial recognition technology to help identify the thieves and make the workplace more secure. The footage would stay on file as long as the company wishes to retain it, and could be used to track various measures of employee attendance and performance. Would this be acceptable to you or not? " /></p>

<p>The aggregated responses illustrate that, for instance, most people are more wary of potential encroachments to their homes than their workplaces. The critical potluck, however, was intended to interrogate more subtle patterns that might be observed in how respondents conceived of that fuzzy line between acceptable and creepy. The task was to pose a question to the survey data designed to generate results that reveal points of tension in how survey respondents constitute their private lives.  Consider the great number of scholars who have examined the boundary between public and private—Michel Foucault, Jurgen Habermas, Nancy Fraser, Michael Warner, and so on. Many of these writers have discussed the relationship between public and private in terms of sites, spaces, or spheres. In <em>The Human Condition</em> (1958), Hannah Arendt claims, “the four walls of one’s private property offer the only reliable hiding place from the common public world, not only from everything that goes on in it but also from its very publicity, from being seen and being heard.”<a href="#fn0"><sup>1</sup></a><a href="#fn0"></a> Private spaces are homes, glove boxes, and safe deposit boxes. What happens to the language and concepts of privacy when it is represented in 1s and 0s and distributed across databases and server farms?</p>

<p>The potluck discussion began with this familiar idea of a private space: Is it strange that a smart thermostat is so much more worrisome than workplace cameras that use facial recognition algorithms? But as we worked through more of the responses, participants became more interested in a peculiar incongruence in the  charged, aggressive manner that respondents discussed such fuzzy, abstract ideas, like their data, information, and even stuff.</p>

<p>Rather than just look at specific lexical patterns, though, we wanted to track what kinds of words and constructions were used to describe emergent notions of privacy. We ended up turning to <a href="http://www.cmu.edu/dietrich/english/research/docuscope.html">Docuscope</a>, a dictionary of over a hundred abstract categories (what the designers call Language Action Types, or LATs) that can classify over 40 million linguistic patterns according to their rhetorical usage. Docuscope was designed by Carnegie Mellon rhetoricians, David Kauffer and Suguru Ishizaki, and they describe it as a tool for, “corpus-based rhetorical analysis.” We uploaded the survey responses to the web-based tool, <a href="http://vep.cs.wisc.edu/ubiq/">Ubiqu+Ity</a>, which tags the text according to the Docuscope categories. Here’s how one tagged response looks:</p>

<p><img src="/assets/docu_tag.png" alt="tag" /></p>

<p>The interface of tagged responses provided more precise vocabulary for describing the relationship between exclamatory adjectives and vague nouns that seemed to recur in the survey answers. <strong>Abstract concepts</strong> is the most frequent LAT in the data, and <strong>Reporting States</strong>, <strong>Negativity</strong>, and <strong>Deny and Disclaim</strong> are also very prevalent. In this example contingent verb constructions shape how the respondent relates to abstract ideas of “information” and “basic activities.”  The respondent considers the different variables at play that would determine whether being monitored via video at work would be acceptable or not.</p>

<p>When the scale of analysis is shifted from individual responses to the LATs in the larger corpus, it appears that the <strong>Contingency</strong> category best explains the variation among the answers. The graph below is a plot of all 1085 text-box responses to the survey. The points are positioned according the frequencies of every LAT categories in each response. This means that each data point initially has over a hundred dimensions in which it could be plotted, but, of course, there’s no way to present this virtual space and make sense of it. Instead, I’ve reduced the space to two dimensions using the statistical technique of Principal Component Analysis, which produce two new composite axes (made up of a combination of the original 100+ axes) that represent the greatest spread of the data, the most variation in values along one axis and then the axis that is orthogonal to the first axis. These are the principal components.</p>

<p><img src="/assets/privacySurvey_pca2.png" alt="PCA1" /></p>

<p>By examining the LAT categories that have the greatest ‘pull’ on the data, we can observe that, along the x-axis, the presence of <strong>Contingency</strong> language category seems to explain that most difference between responses. Points positioned to the left of the zero on the x-axis refer to answers that have virtually no words or phrases tagged as <strong>Contingency</strong>, while most of the points to the right, especially in the blue cluster have a relatively high frequency of contingency-tagged terms. Below are representative responses for different areas of the graph, and you can look at the other LAT categories that contribute to the variation between responses <a href="/pca-features.html">here</a> for more details.). For the rest of this brief analysis, though, I want to consider how contingency operates in the survey answers. This result raises a new question: <strong>What’s at stake in contingently conceiving the contours of one’s private life?</strong> The responses on the right side of the graph indicate participants imaginatively delimiting what ought to be private.</p>

<p><img src="/assets/pca-quote.png" alt="PCA2" /></p>

<p>The <strong>Contingency</strong> tag often refers to responses written in the subjunctive mood, which, as the Oxford English Dictionary reminds us is a grammatical form used to “denote an action or a state as conceived (and not as a fact) and therefore used to express a wish, command, exhortation, or a contingent, hypothetical, or prospective event.” (I like this definition because it acknowledges the use of the subjunctive for both prospective situations and command, which gestures to the angry frustration exhibited in many of these responses.) Psychologist Jerome Bruner observed, “To be in the subjunctive mode is to be trafficking in human possibilities rather than in settled certainties.”<a href="#fn2"><sup>3</sup></a><a href="#fn2"></a> <strong>Contingency</strong> in the responses tends to correlate with <strong>Abstract Concepts</strong> and <strong>Reporting Events</strong>, creating a profile that is similar to the tagged example included above. Here are other representative answers:</p>

<ul>
  <li>“I feel like this information would be used against me if I got into an accident. For example, the insurance agency will look at the data and say that I was speeding right before the accident and claim that the accident was my fault. I also feel as though law enforcement would attempt to use this information in the event of a criminal case.”</li>
  <li>“If they just tracked employee attendance and performance for statistical purposes I would be fine with it. But if they used that footage to punish employees that show up late/goof off then I would not be okay with it.”</li>
</ul>

<p>Part of what is interesting about using the presence of contingent language for separating responses is the fact that taking a suppositional stance turns out not to be very popular perspective, despite the fact that almost all of the questions are phrased in the subjunctive mood. Sixty-eight percent of the respondents did not use any contingent language according to the Docuscope tagger. In the survey, the more common stance is a flat out denial of any demands upon one’s privacy (though this seems unlikely to be as common in real-world decisions). Contingent answers, by contrast, exhibit a willingness to play along. The Pew Research Center administered this survey digitally, with participants recording their answers at computer terminals. Indeed, responses even begin to suspect the survey itself as a privacy violation at times.</p>

<blockquote>
  <p>“I do not want to give any answer to this.”</p>
</blockquote>

<p>This arrangement is fitting since the method of the survey conveniently echoes the typical media interface in which commercial demands are made upon privacy – the screen. Media historian Lisa Gitelman links the plane of the screen to what Walter Benjamin calls the ‘dictatorial perpendicular’ of modern reading.<a href="#fn1"><sup>2</sup></a><a href="#fn1"></a> Locating the screen in its natural post-capitalist habitat, Gitelman explains that “the office walls contain cubicles, the cubicles contain screens, the screens contain windows, and the windows contain page images. These vertical surfaces nest within each other, interfacing like a sequence of Russian dolls, waiting to funnel attention toward documents as if their very perpendicular sequence could ward off distraction” (130).</p>

<p>Even outside of the workplace, screens prescribe conditions of attention and use that proliferate the number and kinds of spaces in which private content might reside. Contingent responses to the Pew Survey index users grappling with the circumstances of new and vulnerable private spaces.</p>

<p><em>One response to a question regarding access to one’s phone camera and location:</em></p>

<blockquote>
  <p>Because it is not a good idea. By giving anyone access to our camera and your location would mean they could activate your camera anytime without your permission. This means they could spy on you any time they wanted to without your knowledge. To me, this would be a huge invasion of privacy.</p>
</blockquote>

<p>In the emergent environment of digital media, privacy cannot properly be delimited to a physical space (although the vehement rejections of any type of home monitoring in the survey responses indicate that this circumstance has not been accepted calmly). Being connected creates a paradoxical condition for privacy. As media theorist Wendy Chun suggests, “Internet users are curiously inside out — they are framed as private subjects exposed in public.”<a href="#fn3"><sup>4</sup></a><a href="#fn3"></a> Digital media is governed by a “logic of containment, which is always imagined as already transgressed” (13). The word <em>imagined</em> is key here. Private material, data, information is distributed, networked, and re-sold to the point that we can really only imagine the borders of our private lives. The concept of privacy might then be more accurately located in the context of performance, play, heuristic, or supposition, rather than in Arendt’s four walls. It is fluid and demands to be continually re-defined, re-constituted, and re-imagined.</p>

<p><em>With significant planning assistance from Gretta Tritch Roman and potluck contributions from Maria Cecire, Adhaar Desai, Miriam Felton-Dansky, and Natalie Desrosiers.</em></p>

<h2 id="references">References</h2>

<p><a id="fn0"></a> 
   1. Hannah Arendt, <em>The Human Condition</em>, 2nd ed. (Chicago: University of Chicago, 1958), 71.</p>

<p><a id="fn1"></a> 
    2. Lisa Gitelman, <em>Paper Knowledge: Toward a Media History of Documents</em>, (Durham, NC: Duke University Press, 2014), 129.</p>

<p><a id="fn2"></a> 
    3. Jeremy Bruner, <em>Actual Minds, Possible Worlds</em>, (Cambridge: Harvard University Press, 1987), 26.</p>

<p><a id="fn3"></a> 
    4. Wendy Hui Kyong Chun, <em>Updating to Remain the Same: Habitual New Media</em>, (Cambridge, MA: MIT Press, 2016), 12.</p>

<table style="border-style: hidden; border-collapse: collapse;">

</table>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/2016/10/04/privacy-performed/</guid>
                <description>
                    
                </description>
                <pubDate>Tue, 04 Oct 2016 10:27:24 -0400</pubDate>
                <author>Collin Jennings</author>
            </item>
        
    
        
            <item>
                <title>The Language of Notation and Imaginative Writing</title>
                <link>http://collinjennings.com/2016/07/22/language-notation/</link>
                <content:encoded>
                    <![CDATA[
                    <p>What are the conditions under which very different words are brought together in writing? Are varied word combinations predisposed to particular genres or discourses? Are there types of words that could be said to constitute lexical situations that would not otherwise occur? And, when words that are not typically used in the same context co-occur, what are they doing? I am going to report the results of a text analysis experiment designed to begin to address these questions. Recent advances in semantic modeling (from topic modeling to word embeddings) make it relatively easy to describe the statistical likelihood of a given set of words to co-occur. Methods in semantic modeling start from the premises that words tend to occur in particular linguistic contexts, and we can decipher the meaning of an unknown word based upon the words that appear near it. While many humanists (like me) interested in text analysis have begun to explore the mathematics of operationalizing these premises, corpus linguists have been thinking about modeling language in this way since the 1940s and 50s. Early theorist of the “distributional structure” of language, Zellig Harris, explains that, “The perennial man in the street believes that when he speaks he freely puts together whatever elements have the meanings he intends; but he does so only by choosing members of those classes that regularly occur together and in the order in which these classes occur.”<a href="#fn0"><sup>1</sup></a><a href="#fn0"></a> His contemporary, J. R. Firth, put the claim even more plainly in his now famous formulation: “You shall know a word by the company it keeps.”<a href="#fn1"><sup>2</sup></a><a href="#fn1"></a> Since these foundational theories, linguists have produced methods for mathematically representing the tendencies of language. In this experiment, I focus on word space models, which represent the distribution of words in a corpus as vectors with hundreds or thousands of dimensions wherein proximity in the vector space corresponds to semantic similarity. The vector position for any given word represents a probabilistic profile regarding the lexical contexts in which one would expect to find that word. As a consequence, word vectors can be added and subtracted to find the words most similar in the model to the resulting composite vector.</p>

<p><em>Two different semantic contexts of ‘bounds’</em> - 
The values appearing next to the resulting words refer to the cosine similarity score between the composite vector (produced by adding the positive word vectors and subtracting the negative one) and the most similar word vectors in the word space model. The score indicates the proximity of the vectors in the model, which should correspond to their semantic similarity—the closer to 1, the more similar words.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s">'bounds'</span><span class="p">,</span> <span class="s">'leaps'</span><span class="p">],</span> <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="s">'limits'</span><span class="p">])[:</span><span class="mi">6</span><span class="p">])</span>
<span class="k">print</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s">'bounds'</span><span class="p">,</span> <span class="s">'limits'</span><span class="p">],</span> <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="s">'leaps'</span><span class="p">])[:</span><span class="mi">6</span><span class="p">])</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>[('skips', 0.5588804483413696), ('leap', 0.5247325897216797), ('leapt', 0.5211300253868103), ('skipping', 0.5187863111495972), ('flings', 0.5138592720031738), ('alights', 0.5134779810905457)]

[('boundaries', 0.6185899376869202), ('limited', 0.595120906829834), ('limit', 0.5744596719741821), ('precincts', 0.5576064586639404), ('confined', 0.5283891558647156), ('confine', 0.5277968645095825)]
</code></pre>
</div>

<p>Any particular phrase or sentence will feature word combinations that more or less correspond to linguistic patterns represented in the word space model. Defining expectation in terms of the proximity between word vectors, what are the contexts in which unexpected words tend to occur? Do they appear in particular places, like those rare neighborhood bars that attract patrons across a wide demographic range? Or are aberrational word groupings brought together by specific terms, like Prince or Beyoncé drawing fans from all backgrounds?</p>

<p>I come to semantic modeling from a literary perspective. I initially started to think about measures for unusual linguistic situations as potentially indexing the appearance of literary tropes and figures. It’s a simplistic hypothesis, but it still seems reasonable to expect that unusual word combinations may be responsible for literal and figurative dimensions of meaning. The term <em>heart</em> often appears in discussions of biology as well as love; thus it may also appear in a context wherein both connotations obtain. Literary scholars have recently used semantic modeling to observe genre differences between fiction, poetry, and non-fiction as well as between sub-genres of the novel or particular forms of poetry during different historical periods.<a href="#fn2"><sup>3</sup></a><a href="#fn2"></a> This experiment draws upon this previous work but frames the critical questions both more narrowly and broadly in particular ways. On the one hand, I’m looking at the co-occurrence of words on the level of the sentence. Given the distribution of words across a large corpus, how likely are the words in a given sentence to co-occur? On the other hand, I’m considering all of the sentences in a corpus (rather than focusing on specific genres) in order to move from mathematical properties of word distributions to observations about how those properties manifest as linguistic functions in different discursive contexts. From this perspective, it is not at all clear that literary texts would stand out from a broader print environment in the ways we might expect. Literary, or imaginative, writing may even appear more formulaic and orderly than written forms that rely on direct, empirical observation. (I’ll be primarily using <em>imaginative writing</em> in place of the term <em>literature</em> since the latter is anachronistic for the seventeenth-century corpus used in this experiment.) According to the literary theorist Roland Barthes, the challenge for the novel writer, for one, concerns, “how to pass from Notation, and so from the Note, to the Novel, from the discontinuous to the flowing (to the continuous, the smooth [au nappé])?”<a href="#fn3"><sup>4</sup></a><a href="#fn3"></a> Notation here refers to annotating one’s experience—recording a series of events and thoughts that appear together without the linguistic architecture that gives language the statistical character that Harris and Firth describe. Translating the discontinuous record of consciousness into linear prose means moving “from the fragment to the nonfragment” (18).</p>

<p>As the title of this post suggests, notation turns out to be very useful for explaining how the most unlikely linguistic situations are produced and organized in my corpus. I’m working with the publicly released portion of the <a href="http://www.textcreationpartnership.org/tcp-eebo/">Early English Books Online Text-Creation Partnership (EEBO-TCP)</a>, which consists of about 25,000 texts printed between 1475 and 1700. For this corpus, the preliminary answer to my opening string of questions is: there are particular terms that appear in the context of highly varied lexical contexts much more frequently than others, and these tend to be notational terms operating in a range of technical discourses (including heraldry, medicine, ornithology, and botany among others). Notation often refers to specific systems of signs and symbols used within a discipline like mathematics or music, but it can also be used in the manner that Barthes uses it above, to mean note-taking or annotating more generally. Most of the terms and contexts that I will present are best characterized by the first definition, but I will be using notation broadly to encompass both possible meanings because they each refer to processes of collecting unlike things—quantities, ideas, or observations—producing what Barthes calls “a layered text, a histology of cutups, a palimpsest.”<a href="#fn4"><sup>5</sup></a><a href="#fn4"></a>  In seventeenth-century English print, the language of notation performs this function by serving as a linguistic framework of arrangement and combination (as in a list of ingredients for a medical recipe) without requiring the grammatical entailments of linear prose. Considered from a literary historical perspective, the linguistic work of notation offers a useful point of contact for thinking about forms of abstraction and comparison, which produce the conditions under which disparate things are brought together in language. In the case of imaginative writing, we typically ascribe such tasks to tropes and figures, but critical theorists like Barthes and more recently literary historians, including Henry Turner, Elaine Freedgood, and Cannon Schmidt have drawn attention to the function of “technical, denotative, and literal” language in literary works.<a href="#fn5"><sup>6</sup></a><a href="#fn5"></a> As I unpack the results of this experiment, I will locate different forms of notation in relation to this critical conversation in order to highlight the imaginative functions of this mode of arranging language.</p>

<p>I produced a 300-dimension word space model of a portion of the EEBO-TCP corpus (all of the texts printed between 1640 and 1700, from the English Civil Wars through the Restoration and Revolution of 1688, consisting of 18,752 texts) using the <a href="https://en.wikipedia.org/wiki/Word2vec"><code class="highlighter-rouge">word2vec</code></a> package in python. (I will refer to this as my Restoration model.)<a href="#fn6"><sup>7</sup></a><a href="#fn6"></a> <code class="highlighter-rouge">Word2Vec</code> uses a “shallow” neural network to generate a predictive word distribution model, rather than a count-based model. This approach makes it more efficient and easier to train, and some computer scientists argue that predictive word space models perform better than count-based ones, but this is a highly contested argument.<a href="#fn7"><sup>8</sup></a><a href="#fn7"></a> The challenge of using a predictive model for dealing with historical semantics, however, is that the training process makes it difficult to track how a term’s vector profile changes over time. To deal with this issue, I’ve taken a five-year subset (from 1678-1682, during the period of the Popish Plot and Exclusion Crisis in England) of the corpus used to train the model, so that I can compare the usage of words in the subset to their vector profiles in the larger Restoration model. The experiment that I present here then considers which words, occurring in texts printed between 1678 and 1682, tend to appear in contexts featuring the most varied combinations of terms relative to the larger Restoration model. While the results only directly refer to the five-year window of the subset, they appear to indicate a pattern that holds for most of the 1670s and 80s, based on other early investigations.</p>

<p>In order to identify these results, I developed an average standard deviation measure (in collaboration with <a href="http://modelingliteraryhistory.org/">Michael Gavin</a>) that takes every context window for a given term in the five-year subset and finds the standard deviation between each vector position across the 300 dimensions for each word vector that co-occurs in a particular context window. The measure returns the standard deviation of the word vectors in a context window of the term, and that measure is averaged across all of the context windows for the term. The average standard deviation value then provides a proxy for determining how varied the lexical contexts tend to be for each term in the subset corpus (normalized to remove rare terms).<a href="#fn8"><sup>9</sup></a><a href="#fn8"></a> I also explored a method that used the average cosine similarity scores between a key term and the words in its context windows, which represents how likely that particular term would appear within its different contexts. The standard deviation measure, however, represents the variation between all of the words of the context window alone. As a consequence, the top scoring terms for the measure are not necessarily words that appear in the widest range of contexts; instead, the top terms are words that tend to appear in the most uncommon contexts in the subset overall. These are the words that most frequently occur at the center of rare word sequences.</p>

<p>To see what this looks like, here are the top 50 terms returned for the average standard deviation measure for the 1678-1682 subset corpus:</p>

<h2 id="top-50-terms-for-average-standard-deviation-measure">Top 50 terms for average standard deviation measure</h2>
<p><img src="/assets/top_words.png" alt="terms" /></p>

<p>Since I am using a seventeenth-century corpus that includes Latin, English, and French texts including non-standardized spelling, these results look a bit inscrutable at first. I’ve gone through the most common texts in which each term appears in the corpus, and here is a breakdown of the different kinds of words in the top 50 that makes it easier to observe similarities across the terms:</p>

<h2 id="word-types-in-the-top-50-terms">Word Types in the Top 50 Terms</h2>
<p><img src="/assets/pieChart.png" alt="pie" /></p>

<p>From these types, I can make some basic observations. First, the terms primarily come from different technical discourses, including medicine, natural philosophy/alchemy, and cooking/agriculture. These tend to be learned contexts featuring a high prevalence of abstruse language. In part because of the association with expert communities, the terms appear in genres that often combine English and Latin. While I was initially tempted to dismiss Latin terms because they appear in a foreign language context (and thus will unavoidably seem more varied in relation to a primarily English corpus), the Latin words are often performing a similar semantic function to the English terms on the list. Of course, the foreign language context is still a factor here, but I will suggest below that it’s not sufficient to explain the placement of the top terms on the list. What’s more is that it’s not practical to simply remove the Latin texts from the corpus because most of the terms are coming from genres that contain Latin and English in the same context, such as botany and ornithology. An unavoidable feature of seventeenth-century English print is that it is a multilingual environment.</p>

<h2 id="cluster-of-top-150-terms-appearing-in-the-most-varied-contexts">Cluster of Top 150 Terms appearing in the Most Varied Contexts</h2>
<p>To gain more context regarding the sources of the terms, I performed K-means cluster analysis on all of the context windows for the top 150 terms. This meant treating all of the context windows for each term as a single document and then seeing which of the ‘term documents’ are the most similar. This is not far off from the logic behind training a word vector model, and it similarly produces a result in which words that appear in similar contexts are located near one another and far from words appearing in dissimilar contexts. In the two-dimensional representation of the clusters, we can see that <em>bushel</em>, <em>bushels</em>, <em>wheat</em>, and <em>barly</em> are near one another in the upper-left corner, and <em>fraction</em>, <em>numerator</em>, and <em>denominator</em> are clustered in the bottom-right corner. In the <code class="highlighter-rouge">word2vec</code> model, <em>fraction</em> returns a similarity score of 0.81 with <em>numerator</em> and 0.82 with <em>denominator</em>, and <em>numerator</em> and <em>denominator</em> return a score of 0.94—all of which indicate very high semantic similarity between the terms and thus suggest that the k-means graph accurately reflects the proximity between terms in the larger word space model.<br />
<img src="/assets/kmeansNotation.png" alt="top terms" /></p>

<p>In k-means clustering (as with many topic modeling algorithms), the user has to select the number of clusters, and here I settled on four after identifying the top texts in which the words appeared in the subset. There appear to be four loose discourses in which these terms primarily occur. The table below breaks down the contents of these discourses:</p>

<table class="mbtablestyle">
  <thead>
    <tr>
      <th style="text-align: center">Cluster</th>
      <th style="text-align: center">Discourse</th>
      <th>Exemplary Titles</th>
      <th style="text-align: center">Top Terms</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>Green</strong></td>
      <td style="text-align: center">Latin texts primarily on botany and natural history</td>
      <td>Robert Morison, <em>Plantarum historiæ universalis Oxoniensis. Pars secunda seu herbarum distributio nova, per tabulas congnationis &amp; affinitatis ex libro naturæ observata &amp; detecta</em> (1680)</td>
      <td style="text-align: center">folio, sine, sit, major, species, minor, anno, sex, alias</td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: center"> </td>
      <td>Sir Robert Sibbald, <em>Scotland illustrated, or, An essay of natural history in which are exquisitely displayed the nature of the country, the dispositions and manners of the inhabitants…and the manifold productions of nature in its three-fold kingdom, (viz.) vegetable, animal and mineral, dispersed throughout the northern part of Great Brittain</em> (1684)</td>
      <td style="text-align: center"> </td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Orange</strong></td>
      <td style="text-align: center">Orinthology</td>
      <td>John Ray, <em>The ornithology of Francis Willughby of Middleton in the county of Warwick Esq, fellow of the Royal Society in three books : wherein all the birds hitherto known, being reduced into a method sutable to their natures, are accurately described</em> (1678)</td>
      <td style="text-align: center">feathers, white, black, colour, middle, tips, dusky, exteriour, outmost</td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: center"> </td>
      <td>John Josselyn, <em>New-Englands rarities discovered in birds, beasts, fishes, serpents, and plants of that country</em> (1672)</td>
      <td style="text-align: center"> </td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Pink</strong></td>
      <td style="text-align: center">Heraldry</td>
      <td>Sir George Mackenzie, <em>The science of herauldry, treated as a part of the civil law, and law of nations wherein reasons are given for its principles, and etymologies for its harder terms.</em> (1680)</td>
      <td style="text-align: center">three, argent, betwixt, azur, sable, bend, within, beareth, quartered</td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: center"> </td>
      <td>Robert Thoroton, <em>The antiquities of Nottinghamshire extracted out of records, original evidences, leiger books, other manuscripts, and authentick authorities : beautified with maps, prospects, and portraictures</em> (1677)</td>
      <td style="text-align: center"> </td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Purple</strong></td>
      <td style="text-align: center">Chemistry, medicine, and recipe books</td>
      <td>Moses Charras, <em>The Royal Pharmacopoea; Galenical and Chemical, according to the practice of the the most eminent and learned physicians of France</em> (1678)</td>
      <td style="text-align: center">half, ounce, two, dram, three, iij, bushels, ounces</td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: center"> </td>
      <td>George Hartman, <em>The true preserver and restorer of health being a choice collection of select and experienced remedies for all distempers incident to men, women, and children: selected from and experienced by the most famous physicians and chyrurgeons in Europe: together with Excellent directions for cookery</em> (1682)</td>
      <td style="text-align: center"> </td>
    </tr>
  </tbody>
</table>
<p><br />
Above I observed the prevalence of terms from technical discourses, and here we can see the kinds of texts in which these discussions occur. The titles indicate writers attempting to share different bodies of knowledge using specific taxonomies and systems of notation, as illustrated in the “science of herauldry” and an “essay of natural history.” The taxonomies are often attributed to a particular person or group, which creates the need to translate an individual system into the terms of a larger domain of shared knowledge. The top terms indexing the most varied contexts for each discourse include adjectives and numbers, but the most common type of terms, appearing across the clusters, are ones that seem to function as common ligatures used in descriptions of different cases and examples positioned within the respective taxonomies and systems. In ornithology, all species of birds have an <em>exterior</em>, <em>feathers</em>, and <em>tips</em> with features that are <em>outmost</em> or in the <em>middle</em>. Likewise, in natural histories written in Latin, different species are arranged in hierarchical structures with <em>major</em> and <em>minor</em> divisions. The purple cluster features a large amount of mathematical (<em>fraction</em>, <em>denominator</em>) and natural philosophical (<em>vitriolated</em>, <em>pulverized</em>) terms but also a significant number of units of measure and symbols (<em>ounces</em>, <em>drams</em>, <em>gallons</em>, <em>iij</em>, <em>aq</em>). While the latter terms belong to specific notational systems, the former also tend to appear in notational contexts.</p>

<p>Turn to particular passages with high frequencies of notational terms, and one can observe the linguistic and epistemological functions that they perform to make highly varied contexts possible. Notational terms provide the linguistic mortar for producing technical conventions—the descriptors, transitions, and abstract units that recur in lists and tables of various early modern knowledge domains. Take the following recipe for a Powder of Crabs-Claws from <em>The Royal Pharmacopoea</em> (1678). The recipe is set apart from the main text as a list, written in Latin and English in side-by-side columns, with each unit of the ingredients expressed in various apothecary symbols. Although in typical conversation or writing a seventeenth-century Briton would be unlikely to mention <em>river crabs-eyes</em>, <em>white amber</em>, <em>deer’s heart-bone</em>, and <em>saffron</em> in the same context, the genre of the recipe and the notational format that accommodates it make these very unlike objects linguistically and literally combinable.</p>

<p><img src="/assets/crabRecipe.png" alt="Crab claw powder screenshot" title="Moyse Charas, *The Royal Pharmacopoea*, (London: 1678)" /></p>

<p>In a radically different context, we can observe key terms from heraldry performing a similar  function in the description of various coats of arms. Like the recipe, the table of noblemen and their coats of arms is represented in a distinctive printed form—set apart and arranged in a orderly manner, and, as in the recipe, there are particular recurring terms indicating the similarities across various coats even if the contents of the individual coats would be unlikely to appear together in a different context. These terms describe common colors and forms: for instance, <em>arg.</em> (the abbreviation for <em>argent</em> meaning <em>silver</em>), <em>passant</em> (the term for animals depicted in a walking position), and <em>azure</em>, as well as other terms, such as <em>betwixt</em> and <em>within</em>, indicate common prepositions used to link the different components of the coats.</p>

<p><img src="/assets/coat_table.png" alt="Coat of arms table" title="Thomas Fuller, *The history of the worthies of England who for parts and learning have been eminent in the several counties : together with an historical narrative of the native commodities and rarities in each county*, (London: 1662)" /></p>

<p>Across disciplinary contexts, notation operates as a form of writing for translating subjective, potentially idiosyncratic observations into shared, intelligible systems of arrangement. Previous discussions of notation from a literary theoretical vantage have emphasized the “reality effect”<a href="#fn9"><sup>10</sup></a><a href="#fn9"></a> produced by including denotative details in fictional contexts, and, more recently, Elaine Freedgood, Cannon Schmitt, and others have explored the potential for “literalist” readings of denotative language in literary texts.<a href="#fn10"><sup>11</sup></a><a href="#fn10"></a> A literalist reading relies on tracking down references to outside knowledge domains, such as discussions of coal mining in Emile Zola’s <em>Germinal</em> or scientific language in George Elliot’s novels. This approach recognizes that insofar as novelists and poets represent characters participating in practices and crafts of the world, they unavoidably and intentionally introduce disciplines foreign from the experience of the reader. Like systems of notation, literary genres bring together disparate ideas and things. In highlighting notational language in non-literary contexts, I’m not suggesting that one should read recipes, blazons, or scientific tables as literary texts. Instead, I’m attempting to point out that notational contexts perform literary functions of combination, abstraction, and comparison, and recognizing such functions suggests new ways forward in describing how literary tropes and figures operate in relation to the layered format of notational lists and charts.</p>

<p>Consider my favorite exemplary passage of notational language, a recipe for a horse-hoof unguent that appears in a 1686 hunting manual, called <em>The Gentleman’s Recreation</em>.
<img src="/assets/horse_unguent.png" alt="passage" title="Nicholas Cox, The Gentleman's Recreation, (London: 1686)" /></p>

<p>The list of ingredients includes items that are familiar to us but would have been relatively novel to seventeenth-century readers (such as <em>turpentine</em>, which seems to have grown in popularity in the period) as well as objects completely foreign to us but very popular in the period (such as <em>trayn oil</em>). (Not to mention ones that are just baffling like <em>dog’s grease</em>, which turns out to be exactly what it sounds like.) These distinctive relationships between the objects in the recipe and our historical perspective indicate the varied meanings of the objects, beyond semantic significance. An item like train oil, which was produced by harvesting the fat from whale or cod, registers multiple dimensions of meaning in social and political domains in addition to linguistic ones. Indeed, historian K. G. Davies reports that in the 1680s, the Dutch government sent almost 2,000 whaling ships to Greenland to catch 10,000 whales for the production of train oil.<a href="#fn11"><sup>12</sup></a><a href="#fn11"></a> The combination of highly varied words in this example supports the idea that the lexical variation accommodated by notational contexts corresponds to related non-linguistic features—so-called real world consequences. The recipe bears out Barthes’s view of notation as “palimpsest,” wherein multiple semantic domains correspond to multiple historical temporalities. The constellation of disparate ingredients brings together the cultural, social, economic, and colonial histories of early modern Europe. At the moment of composition, the recipe conveyed a technique for creating a new substance out of many different ones, but as a historical document it represents a unique juncture, indicating something that could only have been conceived at a specific moment in time.</p>

<p>The larger point is that the linguistic work of gathering together very different words indexes many more types of work that extend beyond the page and are implied by the histories of the objects and systems of notation represented. This linguistic phenomenon represents one answer to the question posed by Bruno Latour, “<em>how do we pack the world into words?</em>”<a href="#fn12"><sup>13</sup></a><a href="#fn12"></a> From a literary studies angle, I would slightly reframe the question to ask, how does imaginative writing pack the world into words similarly or differently from other modes and genres? In a discussion of mathematical thinking and the imagination, Arielle Saiber and Henry Turner posit that “imagination is that faculty of thinking that facilitates movement across systems of explanation that seem irreconcilable, and that, as a consequence, allows for new thoughts, new arguments, and new explanations to occur.”<a href="#fn13"><sup>14</sup></a><a href="#fn13"></a> If this is the case, then what are the linguistic structures and tropes that make this movement possible? This experiment regarding lexical variation and word space models illustrates how the language of notation contributes to such movement. How might we identify other contributors and the imaginative work they accomplish?</p>

<h2 id="references">References</h2>

<p><a id="fn0"></a> 
   1. Zellig Harris, “Distributional Structure,” <em>The Structure of Language: Readings in the Philosophy of Language</em>, ed. Jerry A. Fodor and Jerrold J. Katz (Englewood Cliff, NJ: Prentice-Hall, 1964), 34.</p>

<p><a id="fn1"></a> 
    2. J. R. Firth, <em>Papers in Linguistics,1934–1951</em> (London: Oxford University Press, 1957), 11.</p>

<p><a id="fn2"></a> 
    3. Roland Barthes, <em>The Preparation of the Novel: Lecture Courses and Seminars at the Collège de France</em> [2003], 3rd ed. (New York: Columbia University Press, 2010), 18. Also for an extended analysis of how Barthes’s understanding of notation changed from “The Reality Effect” essay to The Preparation of the Novel lectures, see Rachel Sagner Buurma and Lauren Heffernan, “Notation After ‘The Reality Effect’: Remaking Reference with Roland Barthes and Sheila Heti,” <em>Representations</em> 125.1 (Winter, 2014), 80-102. This essay appears in the fantastic special issue of <em>Representations</em> on denotative and technical language in the novel. I cite the introduction below.</p>

<p><a id="fn3"></a> 
    4. There’s of course a lot of great recent critical work using semantic modeling. For topic modeling, the <a href="http://journalofdigitalhumanities.org/2-1/topic-modeling-and-digital-humanities-by-david-m-blei/"><em>Journal of the Digital Humanities</em></a> 2.1 (Winter, 2012), special issue on the subject is a great place to start. For thinking about, linguistic patterns on the scale of the sentence, see Sarah Allison, et al., <a href="https://litlab.stanford.edu/LiteraryLabPamphlet5.pdf">“Pamphlet 5: Style ate the Scale of the Sentence”</a>,” (June 2013)[lit lab]. Useful introductions to word embeddings include Lynn Cherney’s site word embedding version of <a href="http://www.ghostweather.com/files/word2vecpride/">“<em>Pride and Prejudice</em> and Word Embedding Distance”</a>; Michael Gavin, <a href="http://modelingliteraryhistory.org/2015/09/18/the-arithmetic-of-concepts-a-response-to-peter-de-bolla/">“The Arithmetic of Concepts: A Response to Peter de Bolla”</a>, Sept. 18, 2015; Ben Schmidt, <a href="http://bookworm.benschmidt.org/posts/2015-10-25-Word-Embeddings.html">“Vector Space Models for the Digital Humanities”</a>, Oct. 25, 2015.</p>

<p><a id="fn4"></a> 
    5. Barthes, 18.</p>

<p><a id="fn5"></a> 
    6. See Elaine Freedgood and Cannon Schmitt, “Denotatively, Technically, Literally,” <em>Representations</em> 125.1 (Winter, 2014), 1-14; and Henry Turner, <em>The English Renaissance Stage: Geometry, Poetics, and the Practical Spatial Arts, 1580-1630</em> (Oxford: Oxford University Press, 2006), 114-154.</p>

<p><a id="fn6"></a> 
   7. The model was trained on context windows spanning 5 words on either side of a given term, for words appearing at least 40 times in the corpus. I removed stopwords (using the standard nltk stopword list), but I did not stem or lemmatize the corpus. The model and a csv of the model contents can be downloaded <a href="https://www.dropbox.com/sh/vg0v4evj3ru4ziz/AAA60zxXvN_NYTMtDzmNx508a?dl=0">here</a>. Since I created the model, the Visualizing English Print team has publicly released a <a href="http://graphics.cs.wisc.edu/WP/vep/tcp/">version</a> of the EEBO-TCP corpus with standardized spelling, which, I expect, would produce a more accurate model for word sense disambiguation.</p>

<p><a id="fn7"></a> 
   8. For arguments supporting count-based approaches to semantic modeling, see Omer Levy, et al.,  “Improved Distributional Similarity with Lessons Learned from Word Embeddings,” <em>Transactions of the Association for Computational Linguistics</em> 3 (2015): 211-225; Omer Levy and Yoav Goldberg, “Linguistic Regularities in Sparse and Explicit Word Representations,” <em>Proceedings of the Eighteenth Conference on Computational Language Learning</em> (2014): 171-180. For the rationale behind predictive modeling, see Marco Baroni, et al., “Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors,” <em>Proceedings of Association for Computational Linguistics</em> (2014); as well as the paper that introduced the <code class="highlighter-rouge">word2vec</code> algorithm, Tomas Mikolov, et al., “Efficient Estimation of Word Representations in Vector Space,” (2013), <a href="http://arxiv.org/abs/1301.3781">http://arxiv.org/abs/1301.3781</a>.</p>

<p><a id="fn8"></a> 
    9. There is still a lot work to do in using word distribution models to analyze phrases and sentences. <code class="highlighter-rouge">Word2vec</code> has been used to identify words that tend to appear in phrases. See Mikolov, et al., <a href="https://arxiv.org/abs/1310.4546">“Distributed Representations of Words and Phrases and their Compositionality,”</a> (Oct., 2013). Yet I have not seen persuasive results in using such models to analyze sentence similarity. My own attempts to use <code class="highlighter-rouge">word2vec</code> to analyze sentence similarity in the EEBO-TCP corpus has not worked at all. My standard deviation measure represents a somewhat crude but effective method for looking at the semantic similarities between words in context windows. I’m sure that there will be additional approaches that might be more useful in the future. Here’s just some information on results from using the measure in this experiment. The mean standard deviation value for each word in the subset 1678-1682 corpus (averaged across all of the context windows of each word) is <code class="highlighter-rouge">0.11229</code>. The mean for the top 150 terms that I present here is <code class="highlighter-rouge">0.139412</code>, which is a little more than five standard deviations above the mean. While these terms stand out from the rest, it is of course true that there is only a relative difference between the lexical variation appearing in the context windows of the top terms and those of the rest of the words in the model. Above I focus on the contexts of the top terms, but the terms with the lowest values for the measure also indicate its efficacy. For instance two of the bottom terms are <em>oge</em> and <em>ordeining</em>, which like many of the top terms are difficult to contextualize at first glance. In the subset, <em>Oge</em> primarily occurs in Irish genealogies in which formulaic phrases appear with great frequency, and the resulting context windows contain a lot of repeated words. Similarly, <em>ordeining</em> largely appears in formulaic phrases in religious texts, most notably <em>the power of ordeining others</em>. For a prime example, see Henry Dodwell, <a href="http://quod.lib.umich.edu/e/eebo/A36253.0001.001/1:29?amt2=120;amt3=40;c=eebo;c=eebo2;g=eebogroup;rgn=div1;view=toc;xc=1;q1=ordeining;op2=near;q2=others;op3=near;q3=power"><em>Separation of churches from episcopal government, as practised by the present non-conformists, proved schismatical from such principles as are least controverted and do withal most popularly explain the sinfulness and mischief of schism</em></a> (London: 1679). Finally, using my subsetting method produces the additional issue of reducing the size of the corpus under analysis and thus allowing texts with idiosyncratic linguistic structures to dominate. The subset corpus contains 4,370 texts. I think working with count-based distribution models might make this problem easier to avoid, although it may introduce other challenges regarding the size of the model.</p>

<p><a id="fn9"></a> 
    10. Roland Barthes, “The Reality Effect,” <em>The Rustle of Language</em> (Berkeley and Los Angeles: University of California, 1989), 141.</p>

<p><a id="fn10"></a> 
    11. Freedgood and Schmitt, 5.</p>

<p><a id="fn11"></a> 
    12. K. G. Davies, <em>The North Atlantic World in the Seventeenth Century</em> (Minneapolis: University of Minnesota Press, 1974), 157.</p>

<p><a id="fn12"></a> 
    13. Bruno Latour, “Circulating Reference: Sampling the Soil in the Amazon Forest,” <em>Pandora’s Hope: Essays on the Reality of Science Studies</em> (Cambridge: Harvard University Press, 1999), 24.</p>

<p><a id="fn13"></a> 
    14. Arielle Seiber and Henry Turner, “Mathematics and the Imagination: A Brief Introduction,” <em>Configurations</em> 17 (2009), 12.</p>

<table style="border-style: hidden; border-collapse: collapse;">

</table>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/2016/07/22/language-notation/</guid>
                <description>
                    
                </description>
                <pubDate>Fri, 22 Jul 2016 10:27:24 -0400</pubDate>
                <author>Collin Jennings</author>
            </item>
        
    
        
            <item>
                <title>Mining Your Search Results</title>
                <link>http://collinjennings.com/2016/04/25/mining-search/</link>
                <content:encoded>
                    <![CDATA[
                    <p><img src="/assets/search_logo.jpg" alt="'logo'" /></p>

<p>This tutorial was the third part of the “Search and Surveillance” Workshop series that I created for Bard College. It describes a technique for producing an co-occurrence network between search terms and journals listed in the results for those terms when used to query JSTOR’s Data for Research service. The rationale for producing such a network comes from my interest in the relationship between search and new understandings of disciplinarity. Put simply, how has search re-made our understanding of disciplinary boundaries? I came to this line of inquiry from my work, as a grad assistant, with Lisa Gitelman on the relationship between JSTOR and search. Lisa has published “Searching and Thinking about Searching JSTOR” in the Search Forum from <em>Representations</em> 127.1.</p>

<p>In this tutorial, we will:</p>

<ul>
  <li>Import Search metadata from <em>JSTOR Data for Research</em></li>
  <li>Create a network of search terms and academic journals that appear in the searches.</li>
  <li>Visualize the network graph of the search results in order to view something like the shape of interdisciplinarity in Liu’s essay.</li>
</ul>

<h3 id="import-necessary-libraries-and-functions">Import necessary libraries and functions</h3>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">os.path</span>
<span class="kn">from</span> <span class="nn">os</span> <span class="kn">import</span> <span class="n">listdir</span>
<span class="kn">from</span> <span class="nn">glob</span> <span class="kn">import</span> <span class="n">glob</span>
<span class="kn">import</span> <span class="nn">codecs</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">community</span>
<span class="c">##functions for reading file and filename </span>
<span class="k">def</span> <span class="nf">remove_ext</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
    <span class="c"># insert your code here</span>
    <span class="nb">file</span><span class="p">,</span> <span class="n">ext</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">splitext</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">file</span> 
<span class="k">def</span> <span class="nf">remove_dir</span><span class="p">(</span><span class="n">filepath</span><span class="p">):</span>
    <span class="c"># insert your code here</span>
    <span class="n">direct</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">direct</span> 
<span class="k">def</span> <span class="nf">get_filename</span><span class="p">(</span><span class="n">filepath</span><span class="p">):</span>
    <span class="c"># insert your code here</span>
    <span class="nb">file</span> <span class="o">=</span> <span class="n">remove_ext</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
    <span class="nb">file</span> <span class="o">=</span> <span class="n">remove_dir</span><span class="p">(</span><span class="nb">file</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">file</span>
<span class="k">def</span> <span class="nf">read_corpus_file</span><span class="p">(</span><span class="n">filepath</span><span class="p">):</span>    
    <span class="s">"""Returns the full raw text of the corpus document, in a single string"""</span>
    <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">codecs</span><span class="o">.</span><span class="nb">open</span><span class="p">(</span><span class="n">filepath</span><span class="p">,</span> <span class="s">"r"</span><span class="p">,</span> <span class="s">"utf-8"</span><span class="p">)</span> <span class="c">#open(filepath)</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span> 
    <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">text</span>
<span class="k">def</span> <span class="nf">find_corpus_files</span><span class="p">(</span><span class="n">corpusdirectory</span><span class="p">):</span>
    <span class="s">"""Returns a list of all filenames of corpus files"""</span>
    <span class="k">return</span> <span class="n">glob</span><span class="p">(</span><span class="n">corpusdirectory</span> <span class="o">+</span> <span class="s">"/*.csv"</span><span class="p">)</span> 
<span class="k">def</span> <span class="nf">texts_containing</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">doc_list</span><span class="p">):</span>
    <span class="s">'''Takes a word and a list of documents (stored as word lists). 
    Returns number of words in a list of documents.'''</span>
    <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span> 
    <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">doc_list</span><span class="p">:</span> 
        <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">:</span> 
            <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">count</span> 
<span class="k">def</span> <span class="nf">idf</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">doc_list</span><span class="p">):</span>
    <span class="s">'''Takes a word and a lists of documents, and returns the inverse of the number of 
    documents that the word appears in.'''</span> 
    <span class="k">return</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">doc_list</span><span class="p">)</span><span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">texts_containing</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">doc_list</span><span class="p">)))</span>
<span class="c">## importing file chunk </span>
<span class="n">corpusdirectory</span> <span class="o">=</span> <span class="s">"liu_transcendental_data"</span>
</code></pre>
</div>

<h3 id="import-jstor-csvs">Import JSTOR CSVs</h3>
<p>We are retrieving the the search results 15 bigram searches using the top 6 key terms for Liu’s 2004 <em>Critical Inquiry</em> article, “Transcendental Data: Toward a Cultureal History and Aesthetics of the New Encoded Discourse,” (Autumn, 2004). JSTOR’s top 6 key terms are: <code class="highlighter-rouge">database</code>, <code class="highlighter-rouge">discourse</code>, <code class="highlighter-rouge">content</code>, <code class="highlighter-rouge">management</code>, <code class="highlighter-rouge">document</code>, and <code class="highlighter-rouge">network</code>.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">journal_edgelist</span> <span class="o">=</span> <span class="p">[]</span> 
<span class="n">journal_dict</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">find_corpus_files</span><span class="p">(</span><span class="n">corpusdirectory</span><span class="p">):</span> 
    <span class="n">query</span> <span class="o">=</span> <span class="n">get_filename</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">'_'</span><span class="p">)</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">read_corpus_file</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">():</span>
            <span class="k">continue</span>
        <span class="k">if</span> <span class="s">"Journal,ARTICLE_COUNT"</span> <span class="ow">in</span> <span class="n">line</span><span class="p">:</span>
            <span class="k">continue</span> 
        <span class="k">else</span><span class="p">:</span> 
            <span class="n">x</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s">'utf8'</span><span class="p">)</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf8'</span><span class="p">)</span>
            <span class="n">journal</span><span class="p">,</span><span class="n">number</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">","</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c">#[1]#, 1)</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">query</span><span class="p">:</span> 
                <span class="n">journal_dict</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">journal</span><span class="p">,</span> <span class="n">number</span><span class="p">))</span>
</code></pre>
</div>

<h4 id="check-that-we-have-the-journals-for-each-search-term">Check that we have the journals for each search term.</h4>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">journal_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>dict_keys(['management', 'database', 'discourse', 'network', 'content', 'document'])
</code></pre>
</div>

<h3 id="weight-the-search-results-using-the-term-frequency-inverse-document-frequency-measure">Weight the search results using the Term Frequency-Inverse Document Frequency Measure</h3>
<p>This measure is a popular and conventional measure from Information Retrieval (the field responsible for many search algorithms) used to score terms in a document based on their relative frequency in that document and their inverse frequency in the larger corpus. It thus give higher scores to terms (or here journals) that often appear in one document (or search) and not in others, so as to reduce the noise from terms/journals that are frequent across all documents/searches.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">journals</span> <span class="o">=</span> <span class="p">[]</span> 
<span class="n">journal_corpus</span> <span class="o">=</span> <span class="p">[]</span> 
<span class="k">for</span> <span class="n">query</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">journal_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span> 
    <span class="n">query_list</span> <span class="o">=</span> <span class="p">[]</span> 
    <span class="k">for</span> <span class="n">journal</span><span class="p">,</span> <span class="n">num</span> <span class="ow">in</span> <span class="n">value</span><span class="p">:</span> 
        <span class="n">query_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">journal</span><span class="p">)</span>
        <span class="n">journals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">journal</span><span class="p">)</span>
    <span class="n">journal_corpus</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">query_list</span><span class="p">)</span>
<span class="n">journals</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">journals</span><span class="p">))</span>
</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">journal_idf</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="k">for</span> <span class="n">journal</span> <span class="ow">in</span> <span class="n">journals</span><span class="p">:</span> 
    <span class="n">journal_idf</span><span class="p">[</span><span class="n">journal</span><span class="p">]</span> <span class="o">=</span> <span class="n">idf</span><span class="p">(</span><span class="n">journal</span><span class="p">,</span> <span class="n">journal_corpus</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> 
</code></pre>
</div>

<h4 id="check-the-inverse-document-frequency-of-one-journal">Check the Inverse Document Frequency of one Journal</h4>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">journal_idf</span><span class="p">[</span><span class="s">'The American Historical Review'</span><span class="p">]</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>0.8458493201727416
</code></pre>
</div>

<h4 id="first-we-we-will-get-the-tf-idf-score-for-each-journal">First we we will get the TF-IDF score for each journal</h4>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">journal_network2</span> <span class="o">=</span> <span class="p">[]</span> 
<span class="n">query_list</span> <span class="o">=</span> <span class="p">[]</span> 
<span class="k">for</span> <span class="n">query</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">journal_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">total_value</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c">#for key, value in query.items():</span>
    <span class="n">query_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">journal</span><span class="p">,</span> <span class="n">num</span> <span class="ow">in</span> <span class="n">value</span><span class="p">:</span>
        <span class="n">total_value</span> <span class="o">+=</span> <span class="nb">int</span><span class="p">(</span><span class="n">num</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">journal</span><span class="p">,</span> <span class="n">num</span> <span class="ow">in</span> <span class="n">value</span><span class="p">:</span>
        <span class="n">tf_idf_score</span> <span class="o">=</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">num</span><span class="p">)</span><span class="o">/</span><span class="n">total_value</span><span class="p">)</span> <span class="o">*</span><span class="n">journal_idf</span><span class="p">[</span><span class="n">journal</span><span class="p">]</span>
        <span class="n">journal_network2</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">query</span><span class="p">,</span> <span class="n">journal</span><span class="p">,</span> <span class="n">tf_idf_score</span><span class="p">))</span>
</code></pre>
</div>

<h4 id="now-we-will-get-the-average-score--re-produce-the-network-with-a-filter-to-take-only-journals-that-have-scores-25-times-greater-than-the-average-score">Now we will get the average score &amp; re-produce the network with a filter to take only journals that have scores 2.5 times greater than the average score</h4>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">total_score</span> <span class="o">=</span> <span class="mi">0</span> 
<span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">journal_network2</span><span class="p">:</span>
    <span class="n">total_score</span> <span class="o">+=</span> <span class="n">item</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">avg</span> <span class="o">=</span> <span class="n">total_score</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">journal_network2</span><span class="p">)</span>
</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">journal_network2</span> <span class="o">=</span> <span class="p">[]</span> 
<span class="n">query_list</span> <span class="o">=</span> <span class="p">[]</span> 
<span class="k">for</span> <span class="n">query</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">journal_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">total_value</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c">#for key, value in query.items():</span>
    <span class="n">query_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">journal</span><span class="p">,</span> <span class="n">num</span> <span class="ow">in</span> <span class="n">value</span><span class="p">:</span>
        <span class="n">total_value</span> <span class="o">+=</span> <span class="nb">int</span><span class="p">(</span><span class="n">num</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">journal</span><span class="p">,</span> <span class="n">num</span> <span class="ow">in</span> <span class="n">value</span><span class="p">:</span>
        <span class="n">tf_idf_score</span> <span class="o">=</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">num</span><span class="p">)</span><span class="o">/</span><span class="n">total_value</span><span class="p">)</span> <span class="o">*</span><span class="n">journal_idf</span><span class="p">[</span><span class="n">journal</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">tf_idf_score</span> <span class="o">&gt;</span> <span class="n">avg</span><span class="o">*</span><span class="mf">2.5</span><span class="p">:</span> 
            <span class="n">journal_network2</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">query</span><span class="p">,</span> <span class="n">journal</span><span class="p">,</span> <span class="n">tf_idf_score</span><span class="p">))</span>
</code></pre>
</div>

<h3 id="create-network-graph">Create network graph</h3>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">networkx</span> <span class="kn">as</span> <span class="nn">nx</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">journal_network2</span><span class="p">:</span>
    <span class="n">wt</span> <span class="o">=</span> <span class="n">i</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">*</span><span class="mi">20</span>
    <span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">i</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">i</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">weight</span><span class="o">=</span><span class="n">wt</span><span class="p">)</span>
<span class="n">pos</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">spring_layout</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</code></pre>
</div>

<h3 id="alter-size-of-network-nodes-according-to-number-of-edges">Alter size of network nodes according to number of edges</h3>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">nodes</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">degrees</span> <span class="o">=</span> <span class="p">[]</span> 
<span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">G</span><span class="o">.</span><span class="n">nodes</span><span class="p">():</span> 
    <span class="n">node_degree</span> <span class="o">=</span> <span class="mi">0</span> 
    <span class="k">for</span> <span class="n">edge</span> <span class="ow">in</span> <span class="n">G</span><span class="o">.</span><span class="n">edges</span><span class="p">():</span> 
        <span class="k">if</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">edge</span><span class="p">:</span> 
            <span class="n">node_degree</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">node_degree</span> <span class="o">=</span> <span class="n">node_degree</span><span class="o">*</span><span class="mi">50</span>
    <span class="n">degrees</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node_degree</span><span class="p">)</span>
    <span class="n">nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
</code></pre>
</div>

<h4 id="modify-edge-colors-in-network">Modify edge colors in network</h4>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">colors</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">query_edges</span> <span class="o">=</span> <span class="p">[]</span> 
<span class="k">for</span> <span class="n">edge</span> <span class="ow">in</span> <span class="n">G</span><span class="o">.</span><span class="n">edges</span><span class="p">():</span> 
    <span class="k">if</span> <span class="s">'content'</span> <span class="ow">in</span> <span class="n">edge</span><span class="p">:</span> 
        <span class="n">colors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s">'r'</span><span class="p">)</span>
        <span class="n">query_edges</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">edge</span><span class="p">)</span>
    <span class="k">elif</span> <span class="s">'discourse'</span> <span class="ow">in</span> <span class="n">edge</span><span class="p">:</span> 
        <span class="n">colors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s">'b'</span><span class="p">)</span>
        <span class="n">query_edges</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">edge</span><span class="p">)</span>
    <span class="k">elif</span> <span class="s">'database'</span> <span class="ow">in</span> <span class="n">edge</span><span class="p">:</span> 
        <span class="n">colors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s">'g'</span><span class="p">)</span>
        <span class="n">query_edges</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">edge</span><span class="p">)</span>
    <span class="k">elif</span> <span class="s">'management'</span> <span class="ow">in</span> <span class="n">edge</span><span class="p">:</span> 
        <span class="n">colors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s">'m'</span><span class="p">)</span>
        <span class="n">query_edges</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">edge</span><span class="p">)</span>
    <span class="k">elif</span> <span class="s">'document'</span> <span class="ow">in</span> <span class="n">edge</span><span class="p">:</span> 
        <span class="n">colors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s">'y'</span><span class="p">)</span>
        <span class="n">query_edges</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">edge</span><span class="p">)</span>
    <span class="k">elif</span> <span class="s">'network'</span> <span class="ow">in</span> <span class="n">edge</span><span class="p">:</span> 
        <span class="n">colors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s">'c'</span><span class="p">)</span>
        <span class="n">query_edges</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">edge</span><span class="p">)</span>
<span class="n">color_labels</span> <span class="o">=</span> <span class="p">[</span><span class="s">'r'</span><span class="p">,</span> <span class="s">'b'</span><span class="p">,</span> <span class="s">'g'</span><span class="p">,</span> <span class="s">'m'</span><span class="p">,</span> <span class="s">'y'</span><span class="p">,</span> <span class="s">'c'</span><span class="p">]</span>
<span class="n">edge_labels</span> <span class="o">=</span> <span class="p">[</span><span class="s">'content'</span><span class="p">,</span> <span class="s">'discourse'</span><span class="p">,</span> <span class="s">'database'</span><span class="p">,</span> <span class="s">'management'</span><span class="p">,</span> <span class="s">'document'</span><span class="p">,</span> <span class="s">'network'</span><span class="p">]</span>   
</code></pre>
</div>

<h2 id="plot--save-the-graph-as-png-file">Plot &amp; Save the graph as png file</h2>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">G</span><span class="o">.</span><span class="n">subgraph</span><span class="p">([</span><span class="n">label</span> <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">pos</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">query_list</span><span class="p">])</span> <span class="c"># if "topic " in topic])</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx_labels</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span>  <span class="n">font_color</span><span class="o">=</span><span class="s">'black'</span><span class="p">,</span> <span class="n">font_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="p">)</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">G</span><span class="o">.</span><span class="n">subgraph</span><span class="p">([</span><span class="n">journal</span> <span class="k">for</span> <span class="n">journal</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">pos</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">journal</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">query_list</span> <span class="ow">and</span> <span class="n">value</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">avg</span><span class="o">*</span><span class="mi">2</span> <span class="ow">or</span> 
               <span class="n">value</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">&gt;</span><span class="n">avg</span><span class="o">*</span><span class="mi">2</span><span class="p">])</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx_labels</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">label_pos</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">font_size</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>

<span class="c">## Algorithm for automatically detecting communities in the network</span>
<span class="n">part</span> <span class="o">=</span> <span class="n">community</span><span class="o">.</span><span class="n">best_partition</span><span class="p">(</span><span class="n">G</span><span class="p">)</span>
<span class="n">values</span> <span class="o">=</span> <span class="p">[</span><span class="n">part</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">node</span><span class="p">)</span> <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">G</span><span class="o">.</span><span class="n">nodes</span><span class="p">()]</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx_nodes</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">node_list</span><span class="o">=</span><span class="n">nodes</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s">'jet'</span><span class="p">),</span> <span class="n">node_color</span> <span class="o">=</span> <span class="n">values</span><span class="p">,</span> <span class="n">node_size</span><span class="o">=</span><span class="n">degrees</span><span class="p">,</span> 
                       <span class="n">with_labels</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="n">edges</span> <span class="o">=</span> <span class="n">G</span><span class="o">.</span><span class="n">edges</span><span class="p">()</span>
<span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">G</span><span class="p">[</span><span class="n">u</span><span class="p">][</span><span class="n">v</span><span class="p">][</span><span class="s">'weight'</span><span class="p">]</span> <span class="k">for</span> <span class="n">u</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">edges</span><span class="p">]</span>
   
    <span class="c"># plot edges</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx_edges</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">edgelist</span><span class="o">=</span><span class="n">G</span><span class="o">.</span><span class="n">edges</span><span class="p">(),</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">edge_color</span><span class="o">=</span><span class="n">colors</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>

<span class="n">f</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">color</span><span class="p">,</span> <span class="n">edge</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">color_labels</span><span class="p">,</span> <span class="n">edge_labels</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="n">edge</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Co-occuring Journal Titles in Search Results for Top 6 Key Terms in Liu's 'Transcendental Data'"</span><span class="p">,</span>
          <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">'search_network.png'</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">orientation</span><span class="o">=</span><span class="s">'landscape'</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s">'tight'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>/Users/collin/anaconda/lib/python3.4/site-packages/matplotlib/collections.py:650: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison
  if self._edgecolors_original != str('face'):
/Users/collin/anaconda/lib/python3.4/site-packages/matplotlib/collections.py:590: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison
  if self._edgecolors == str('face'):
</code></pre>
</div>

<p><img src="/assets/output_26_1.png" alt="terms" /></p>


                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/2016/04/25/mining-search/</guid>
                <description>
                    
                </description>
                <pubDate>Mon, 25 Apr 2016 10:27:24 -0400</pubDate>
                <author>Collin Jennings</author>
            </item>
        
    
  </channel>
</rss>
